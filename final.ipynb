{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "import operator\n",
    "import nltk\n",
    "import sklearn\n",
    "import h5py\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.display import display\n",
    "import random "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "identity_columns = ['asian', 'atheist', 'bisexual', 'black', 'buddhist', 'christian', 'female', \n",
    "                    'heterosexual', 'hindu',\n",
    "                    'homosexual_gay_or_lesbian', 'intellectual_or_learning_disability',\n",
    "                    'jewish', 'latino', 'male', 'muslim', 'transgender', 'white']\n",
    "\n",
    "\n",
    "identity_columns2 = ['_asian', '_atheist', '_bisexual', '_black', '_buddhist', '_christian', '_female', \n",
    "                    '_heterosexual', '_hindu',\n",
    "                    '_homosexual_gay_or_lesbian', '_intellectual_or_learning_disability',\n",
    "                    '_jewish', '_latino', '_male', '_muslim', '_transgender', '_white']\n",
    "keep = ['comment_text','asian', 'atheist', 'bisexual',\n",
    "       'black', 'buddhist', 'christian', 'female', 'heterosexual', 'hindu',\n",
    "       'homosexual_gay_or_lesbian', 'intellectual_or_learning_disability',\n",
    "       'jewish', 'latino', 'male', 'muslim', 'transgender', 'white']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw = data_raw.dropna()\n",
    "y = data_raw.target\n",
    "X = data_raw[keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "X['comment_text'] = X['comment_text'].astype(str)\n",
    "\n",
    "for col in identity_columns:\n",
    "    X[col] = np.where(X[col] >= 0.5, True, False)\n",
    "\n",
    "y = np.where(y >= 0.5, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.reset_index(drop = True, inplace = True)\n",
    "tempy = pd.DataFrame(y.copy())\n",
    "tempy.columns = ['toxicity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicity_rate_per_identity = np.zeros(len(identity_columns))\n",
    "for i in range(len(identity_columns)):\n",
    "    c = identity_columns[i]\n",
    "    temp1 = X[X[c] == True]\n",
    "    n = len(temp1)\n",
    "    \n",
    "    temp2 = tempy.loc[list(temp1.index)]\n",
    "    k = len(temp2[temp2['toxicity'] == True])\n",
    "    toxicity_rate_per_identity[i] = k/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.11666667, 0.10851927, 0.21176471, 0.30497164, 0.09972299,\n",
       "       0.08101616, 0.12909184, 0.21841542, 0.1043956 , 0.27434354,\n",
       "       0.09615385, 0.14920319, 0.18704453, 0.14298477, 0.21215296,\n",
       "       0.19750367, 0.27636831])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxicity_rate_per_identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test_real, y_train, y_test = train_test_split(X, y, test_size=0.20)\n",
    "X_test = X_test_real['comment_text']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(toxicity_rate_per_identity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(188069, 1014)"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# edited from preprocessSentences.py script from HW1\n",
    "import nltk, re, pprint\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from os import listdir\n",
    "from os.path import isfile, isdir, join\n",
    "import numpy\n",
    "import re\n",
    "import sys\n",
    "import getopt\n",
    "import codecs\n",
    "import time\n",
    "import os\n",
    "import csv\n",
    "\n",
    "chars = ['{','}','#','%','&','\\(','\\)','\\[','\\]','<','>',',', '!', '.', ';', \n",
    "'?', '*', '\\\\', '\\/', '~', '_','|','=','+','^',':','\\\"','\\'','@','-']\n",
    "\n",
    "def tokenize_corpus(essay, train=True):\n",
    "\n",
    "  porter = nltk.PorterStemmer() # also lancaster stemmer\n",
    "  wnl = nltk.WordNetLemmatizer()\n",
    "  stopWords = stopwords.words(\"english\")\n",
    "  classes = []\n",
    "  samples = []\n",
    "  docs = []\n",
    "  if train == True:\n",
    "    words = {}\n",
    "\n",
    "  lines = essay.tolist()\n",
    "\n",
    "  for line in lines:\n",
    "    raw = line\n",
    "    raw = ' '.join(raw.rsplit()[1:-1])\n",
    "    # remove noisy characters; tokenize\n",
    "    raw = re.sub(r'\\n', ' ', raw)\n",
    "    raw = re.sub(\"<\", \"(\", raw)\n",
    "    raw = re.sub(\">\", \")\", raw)\n",
    "    raw = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", raw)\n",
    "    raw = re.sub('[%s]' % ''.join(chars), '', raw)\n",
    "    #print(raw)\n",
    "    #raw = re.sub('\\n', '', raw)\n",
    "    tokens = word_tokenize(raw)\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    tokens = [w for w in tokens if w not in stopWords]\n",
    "    tokens = [wnl.lemmatize(t) for t in tokens]\n",
    "    tokens = [porter.stem(t) for t in tokens]   \n",
    "    if train == True:\n",
    "     for t in tokens: \n",
    "         try:\n",
    "             words[t] = words[t]+1\n",
    "         except:\n",
    "             words[t] = 1\n",
    "    docs.append(tokens)\n",
    "\n",
    "  if train == True:\n",
    "     return(docs, classes, samples, words)\n",
    "  else:\n",
    "     return(docs, classes, samples)\n",
    "\n",
    "\n",
    "def wordcount_filter(words, num=1):\n",
    "   keepset = []\n",
    "   for k in words.keys():\n",
    "       if(words[k] > num):\n",
    "           keepset.append(k)\n",
    "   print (\"Vocab length:\", len(keepset))\n",
    "   return(sorted(set(keepset)))\n",
    "\n",
    "\n",
    "def find_wordcounts(docs, vocab):\n",
    "   bagofwords = numpy.zeros(shape=(len(docs),len(vocab)), dtype=numpy.uint8)\n",
    "   vocabIndex={}\n",
    "   for i in range(len(vocab)):\n",
    "      vocabIndex[vocab[i]]=i\n",
    "\n",
    "   for i in range(len(docs)):\n",
    "       doc = docs[i]\n",
    "\n",
    "       for t in doc:\n",
    "          index_t=vocabIndex.get(t)\n",
    "          if index_t != None:\n",
    "             bagofwords[i,index_t]=bagofwords[i,index_t]+1\n",
    "\n",
    "   print (\"Finished find_wordcounts for:\", len(docs), \"docs\")\n",
    "   return(bagofwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "(docs, classes, samples, words) = tokenize_corpus(X_train.comment_text, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab length: 997\n",
      "word and word count threshold is:\n",
      "('threaten', 1101)\n"
     ]
    }
   ],
   "source": [
    "sorted_x = sorted(words.items(), key=operator.itemgetter(1))\n",
    "word_count_threshold = sorted_x[-1000][1]\n",
    "vocab = wordcount_filter(words, num=word_count_threshold)\n",
    "\n",
    "print('word and word count threshold is:')\n",
    "print(sorted_x[-1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished find_wordcounts for: 188069 docs\n"
     ]
    }
   ],
   "source": [
    "bow1 = find_wordcounts(docs, vocab)\n",
    "df1 = pd.DataFrame(bow1, columns = vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished find_wordcounts for: 47018 docs\n"
     ]
    }
   ],
   "source": [
    "#####for noidentitybias analysis #####\n",
    "# we have X train called 'df1' and y train called y_train\n",
    "# we now need to create X_test using the vocabulary selected from X_train, aka the columns of df1\n",
    "# fit df1 using y_train, use the fitted model to predict y_test from X_test, then compare with real values of y_test\n",
    "\n",
    "\n",
    "#we have X_test, where each row is a comment. \n",
    "# we generate new list of words in each comment in X_test\n",
    "# we find the wordcounts of the new \"docs\" using the vocab list generated by X_train\n",
    "\n",
    "(docs2, classes2, samples2, words2) = tokenize_corpus(X_test, train=True)\n",
    "bow2 = find_wordcounts(docs2, vocab)\n",
    "df2 = pd.DataFrame(bow2, columns = vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "X_test_notext = X_test_real.copy().drop(['comment_text'], axis=1)\n",
    "s = X_test_notext.columns.to_series()\n",
    "X_test_notext.columns = ('_')+s\n",
    "X_test_notext.reset_index(drop=True, inplace=True)\n",
    "X_test_final = pd.concat([X_test_notext, df2], axis=1)\n",
    "\n",
    "\n",
    "X_train_notext = X_train.copy().drop(['comment_text'], axis=1)\n",
    "s = X_train_notext.columns.to_series()\n",
    "X_train_notext.columns = ('_')+s\n",
    "X_train_notext.reset_index(drop=True, inplace=True)\n",
    "X_train_final = pd.concat([X_train_notext, df1], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7159682347926457\n",
      "0.7720942845445022\n",
      "0.7706410310944745\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "clf_log_noidentity = LogisticRegression(random_state=0, solver='lbfgs').fit(X_train_final.copy().drop(identity_columns2, axis = 1), y_train)\n",
    "    \n",
    "y_predict_log_noidentity = clf_log_noidentity.predict(X_test_final.copy().drop(identity_columns2, axis = 1))\n",
    "\n",
    "log_score_noidentity = roc_auc_score(y_test, y_predict_log_noidentity)\n",
    "\n",
    "print(log_score_noidentity)\n",
    "print(clf_log_noidentity.score(X_train_final.copy().drop(identity_columns2, axis=1), y_train))\n",
    "print(clf_log_noidentity.score(X_test_final.copy().drop(identity_columns2, axis = 1), y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "clf_rfc_noidentity = RandomForestClassifier(n_estimators = 50, max_depth = 3).fit(X_train_final.copy().drop(identity_columns2, axis = 1), y_train)\n",
    "    \n",
    "y_predict_rfc_noidentity = clf_rfc_noidentity.predict(X_test_final.copy().drop(identity_columns2, axis = 1))\n",
    "\n",
    "rfc_score_noidentity = roc_auc_score(y_test, y_predict_rfc_noidentity)\n",
    "\n",
    "print(rfc_score_noidentity)\n",
    "print(clf_rfc_noidentity.score(X_train_final.copy().drop(identity_columns2, axis=1), y_train))\n",
    "print(clf_rfc_noidentity.score(X_test_final.copy().drop(identity_columns2, axis = 1), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "clf_svc_noidentity = LinearSVC().fit(X_train_final.copy().drop(identity_columns2, axis = 1), y_train)\n",
    "    \n",
    "y_predict_svc_noidentity = clf_svc_noidentity.predict(X_test_final.copy().drop(identity_columns2, axis = 1))\n",
    "\n",
    "svc_score_noidentity = roc_auc_score(y_test, y_predict_svc_noidentity)\n",
    "\n",
    "print(svc_score_noidentity)\n",
    "print(clf_svc_noidentity.score(X_train_final.copy().drop(identity_columns2, axis=1), y_train))\n",
    "print(clf_svc_noidentity.score(X_test_final.copy().drop(identity_columns2, axis = 1), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall AUC is:\n",
      "0.7159682347926457\n",
      "Subgroup AUC is:\n",
      "0.6953131508730317\n"
     ]
    }
   ],
   "source": [
    "###########compute overall AUC #######\n",
    "print('Overall AUC is:')\n",
    "print(log_score_noidentity)\n",
    "\n",
    "##########LOG#\n",
    "###########compute overall AUC #######\n",
    "print('Overall AUC is:')\n",
    "print(log_score_noidentity)\n",
    "\n",
    "############compute Subgroup AUC #####\n",
    "subgroupAUC_log = np.zeros(len(identity_columns))\n",
    "\n",
    "for i in range(len(identity_columns2)):\n",
    "    c = identity_columns2[i]\n",
    "    #print(c)\n",
    "    temp = X_test_final[X_test_final[c] == True]\n",
    "    #print(temp.shape)\n",
    "    if len(temp) == 0:\n",
    "        subgroupAUC_log[i] = 1\n",
    "    else:\n",
    "        X_test_final_sub1 = temp.drop(identity_columns2, axis = 1)\n",
    "        y_predict_sub1 = clf_log_noidentity.predict(X_test_final_sub1)\n",
    "        y_test_sub1 = y_test[list(X_test_final_sub1.index)]\n",
    "        log_score_sub1 = roc_auc_score(y_test_sub1, y_predict_sub1)\n",
    "        subgroupAUC_log[i] = log_score_sub1\n",
    "\n",
    "            \n",
    "    \n",
    "subgroupAUC_log_mean = np.mean(subgroupAUC_log)\n",
    "print('Subgroup AUC is:')\n",
    "print(subgroupAUC_log_mean)\n",
    "###compute Subgroup AUC #####\n",
    "subgroupAUC_log = np.zeros(len(identity_columns))\n",
    "\n",
    "for i in range(len(identity_columns2)):\n",
    "    c = identity_columns2[i]\n",
    "    #print(c)\n",
    "    temp = X_test_final[X_test_final[c] == True]\n",
    "    #print(temp.shape)\n",
    "    if len(temp) == 0:\n",
    "        subgroupAUC_log[i] = 1\n",
    "    else:\n",
    "        X_test_final_sub1 = temp.drop(identity_columns2, axis = 1)\n",
    "        y_predict_sub1 = clf_log_noidentity.predict(X_test_final_sub1)\n",
    "        y_test_sub1 = y_test[list(X_test_final_sub1.index)]\n",
    "        log_score_sub1 = roc_auc_score(y_test_sub1, y_predict_sub1)\n",
    "        subgroupAUC_log[i] = log_score_sub1\n",
    "\n",
    "            \n",
    "    \n",
    "subgroupAUC_log_mean = np.mean(subgroupAUC_log)\n",
    "print('Subgroup AUC is:')\n",
    "print(subgroupAUC_log_mean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RFC#\n",
    "############compute Subgroup AUC #####\n",
    "subgroupAUC_rfc = np.zeros(len(identity_columns))\n",
    "\n",
    "for i in range(len(identity_columns2)):\n",
    "    c = identity_columns2[i]\n",
    "    #print(c)\n",
    "    temp = X_test_final[X_test_final[c] == True]\n",
    "    #print(temp.shape)\n",
    "    if len(temp) == 0:\n",
    "        subgroupAUC_rfc[i] = 1\n",
    "    else:\n",
    "        X_test_final_sub1 = temp.drop(identity_columns2, axis = 1)\n",
    "        y_predict_sub1 = clf_rfc_noidentity.predict(X_test_final_sub1)\n",
    "        y_test_sub1 = y_test[list(X_test_final_sub1.index)]\n",
    "        rfc_score_sub1 = roc_auc_score(y_test_sub1, y_predict_sub1)\n",
    "        subgroupAUC_rfc[i] = rfc_score_sub1\n",
    "\n",
    "            \n",
    "    \n",
    "subgroupAUC_rfc_mean = np.mean(subgroupAUC_rfc)\n",
    "print('Subgroup AUC is:')\n",
    "print(subgroupAUC_rfc_mean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVC#\n",
    "############compute Subgroup AUC #####\n",
    "subgroupAUC_svc = np.zeros(len(identity_columns))\n",
    "\n",
    "for i in range(len(identity_columns2)):\n",
    "    c = identity_columns2[i]\n",
    "    #print(c)\n",
    "    temp = X_test_final[X_test_final[c] == True]\n",
    "    #print(temp.shape)\n",
    "    if len(temp) == 0:\n",
    "        subgroupAUC_svc[i] = 1\n",
    "    else:\n",
    "        X_test_final_sub1 = temp.drop(identity_columns2, axis = 1)\n",
    "        y_predict_sub1 = clf_svc_noidentity.predict(X_test_final_sub1)\n",
    "        y_test_sub1 = y_test[list(X_test_final_sub1.index)]\n",
    "        svc_score_sub1 = roc_auc_score(y_test_sub1, y_predict_sub1)\n",
    "        subgroupAUC_svc[i] = svc_score_sub1\n",
    "\n",
    "            \n",
    "    \n",
    "subgroupAUC_svc_mean = np.mean(subgroupAUC_svc)\n",
    "print('Subgroup AUC is:')\n",
    "print(subgroupAUC_svc_mean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOG#\n",
    "############compute BPSN Background Positive, Subgroup Negative AUC#####\n",
    "BPSN_log = np.zeros(len(identity_columns))\n",
    "y_test1 = pd.DataFrame(y_test)\n",
    "y_test1.columns = ['toxicity']\n",
    "for i in range(len(identity_columns2)):\n",
    "    c = identity_columns2[i]\n",
    "    temp1 = y_test1[y_test1['toxicity'] == False]\n",
    "    temp2 = X_test_final.loc[list(temp1.index)]\n",
    "    temp_pt1 = temp2[temp2[c] == True]\n",
    "    \n",
    "    temp3 = y_test1[y_test1['toxicity'] == True]\n",
    "    temp4 = X_test_final.loc[list(temp3.index)]\n",
    "    temp_pt2 = temp4[temp4[c] == False]\n",
    "    \n",
    "    X_test_final_sub2 = pd.concat([temp_pt1, temp_pt2], axis = 0)\n",
    "    X_test_final_sub2 = X_test_final_sub2.drop(identity_columns2, axis =1)\n",
    "    \n",
    "    y_predict_sub2 = clf_log_noidentity.predict(X_test_final_sub2)\n",
    "    y_test_sub2 = y_test[list(X_test_final_sub2.index)]\n",
    "    \n",
    "    log_score_sub2 = roc_auc_score(y_test_sub2, y_predict_sub2)\n",
    "    BPSN_log[i] = log_score_sub2\n",
    "    \n",
    "BPSN_log_mean = np.mean(BPSN_log)\n",
    "print('BPSN is:')\n",
    "print(BPSN_log_mean)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RFC#\n",
    "############compute BPSN Background Positive, Subgroup Negative AUC#####\n",
    "BPSN_rfc = np.zeros(len(identity_columns))\n",
    "y_test1 = pd.DataFrame(y_test)\n",
    "y_test1.columns = ['toxicity']\n",
    "for i in range(len(identity_columns2)):\n",
    "    c = identity_columns2[i]\n",
    "    temp1 = y_test1[y_test1['toxicity'] == False]\n",
    "    temp2 = X_test_final.loc[list(temp1.index)]\n",
    "    temp_pt1 = temp2[temp2[c] == True]\n",
    "    \n",
    "    temp3 = y_test1[y_test1['toxicity'] == True]\n",
    "    temp4 = X_test_final.loc[list(temp3.index)]\n",
    "    temp_pt2 = temp4[temp4[c] == False]\n",
    "    \n",
    "    X_test_final_sub2 = pd.concat([temp_pt1, temp_pt2], axis = 0)\n",
    "    X_test_final_sub2 = X_test_final_sub2.drop(identity_columns2, axis =1)\n",
    "    \n",
    "    y_predict_sub2 = clf_rfc_noidentity.predict(X_test_final_sub2)\n",
    "    y_test_sub2 = y_test[list(X_test_final_sub2.index)]\n",
    "    \n",
    "    rfc_score_sub2 = roc_auc_score(y_test_sub2, y_predict_sub2)\n",
    "    BPSN_rfc[i] = rfc_score_sub2\n",
    "    \n",
    "BPSN_rfc_mean = np.mean(BPSN_rfc)\n",
    "print('BPSN is:')\n",
    "print(BPSN_rfc_mean)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVC#\n",
    "############compute BPSN Background Positive, Subgroup Negative AUC#####\n",
    "BPSN_svc = np.zeros(len(identity_columns))\n",
    "y_test1 = pd.DataFrame(y_test)\n",
    "y_test1.columns = ['toxicity']\n",
    "for i in range(len(identity_columns2)):\n",
    "    c = identity_columns2[i]\n",
    "    temp1 = y_test1[y_test1['toxicity'] == False]\n",
    "    temp2 = X_test_final.loc[list(temp1.index)]\n",
    "    temp_pt1 = temp2[temp2[c] == True]\n",
    "    \n",
    "    temp3 = y_test1[y_test1['toxicity'] == True]\n",
    "    temp4 = X_test_final.loc[list(temp3.index)]\n",
    "    temp_pt2 = temp4[temp4[c] == False]\n",
    "    \n",
    "    X_test_final_sub2 = pd.concat([temp_pt1, temp_pt2], axis = 0)\n",
    "    X_test_final_sub2 = X_test_final_sub2.drop(identity_columns2, axis =1)\n",
    "    \n",
    "    y_predict_sub2 = clf_svc_noidentity.predict(X_test_final_sub2)\n",
    "    y_test_sub2 = y_test[list(X_test_final_sub2.index)]\n",
    "    \n",
    "    svc_score_sub2 = roc_auc_score(y_test_sub2, y_predict_sub2)\n",
    "    BPSN_svc[i] = svc_score_sub2\n",
    "    \n",
    "BPSN_svc_mean = np.mean(BPSN_svc)\n",
    "print('BPSN is:')\n",
    "print(BPSN_svc_mean)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOG#\n",
    "############compute BNSP Background Negative, Subgroup Positive AUC#####\n",
    "BNSP_log = np.zeros(len(identity_columns))\n",
    "for i in range(len(identity_columns2)):\n",
    "    c = identity_columns2[i]\n",
    "    temp1 = y_test1[y_test1['toxicity'] == True]\n",
    "    temp2 = X_test_final.loc[list(temp1.index)]\n",
    "    temp_pt1 = temp2[temp2[c] == True]\n",
    "    \n",
    "    temp3 = y_test1[y_test1['toxicity'] == False]\n",
    "    temp4 = X_test_final.loc[list(temp3.index)]\n",
    "    temp_pt2 = temp4[temp4[c] == False]\n",
    "    \n",
    "    X_test_final_sub2 = pd.concat([temp_pt1, temp_pt2], axis = 0)\n",
    "    X_test_final_sub2 = X_test_final_sub2.drop(identity_columns2, axis = 1)\n",
    "    \n",
    "    y_predict_sub2 = clf_log_noidentity.predict(X_test_final_sub2)\n",
    "    y_test_sub2 = y_test[list(X_test_final_sub2.index)]\n",
    "    \n",
    "    log_score_sub2 = roc_auc_score(y_test_sub2, y_predict_sub2)\n",
    "    BNSP_log[i] = log_score_sub2\n",
    "    \n",
    "BNSP_log_mean = np.mean(BNSP_log)\n",
    "print('BNSP is:')\n",
    "print(BNSP_log_mean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RFC#\n",
    "############compute BNSP Background Negative, Subgroup Positive AUC#####\n",
    "BNSP_rfc = np.zeros(len(identity_columns))\n",
    "for i in range(len(identity_columns2)):\n",
    "    c = identity_columns2[i]\n",
    "    temp1 = y_test1[y_test1['toxicity'] == True]\n",
    "    temp2 = X_test_final.loc[list(temp1.index)]\n",
    "    temp_pt1 = temp2[temp2[c] == True]\n",
    "    \n",
    "    temp3 = y_test1[y_test1['toxicity'] == False]\n",
    "    temp4 = X_test_final.loc[list(temp3.index)]\n",
    "    temp_pt2 = temp4[temp4[c] == False]\n",
    "    \n",
    "    X_test_final_sub2 = pd.concat([temp_pt1, temp_pt2], axis = 0)\n",
    "    X_test_final_sub2 = X_test_final_sub2.drop(identity_columns2, axis = 1)\n",
    "    \n",
    "    y_predict_sub2 = clf_rfc_noidentity.predict(X_test_final_sub2)\n",
    "    y_test_sub2 = y_test[list(X_test_final_sub2.index)]\n",
    "    \n",
    "    rfc_score_sub2 = roc_auc_score(y_test_sub2, y_predict_sub2)\n",
    "    BNSP_rfc[i] = rfc_score_sub2\n",
    "    \n",
    "BNSP_rfc_mean = np.mean(BNSP_rfc)\n",
    "print('BNSP is:')\n",
    "print(BNSP_rfc_mean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVC#\n",
    "############compute BNSP Background Negative, Subgroup Positive AUC#####\n",
    "BNSP_svc = np.zeros(len(identity_columns))\n",
    "for i in range(len(identity_columns2)):\n",
    "    c = identity_columns2[i]\n",
    "    temp1 = y_test1[y_test1['toxicity'] == True]\n",
    "    temp2 = X_test_final.loc[list(temp1.index)]\n",
    "    temp_pt1 = temp2[temp2[c] == True]\n",
    "    \n",
    "    temp3 = y_test1[y_test1['toxicity'] == False]\n",
    "    temp4 = X_test_final.loc[list(temp3.index)]\n",
    "    temp_pt2 = temp4[temp4[c] == False]\n",
    "    \n",
    "    X_test_final_sub2 = pd.concat([temp_pt1, temp_pt2], axis = 0)\n",
    "    X_test_final_sub2 = X_test_final_sub2.drop(identity_columns2, axis = 1)\n",
    "    \n",
    "    y_predict_sub2 = clf_svc_noidentity.predict(X_test_final_sub2)\n",
    "    y_test_sub2 = y_test[list(X_test_final_sub2.index)]\n",
    "    \n",
    "    svc_score_sub2 = roc_auc_score(y_test_sub2, y_predict_sub2)\n",
    "    BNSP_svc[i] = svc_score_sub2\n",
    "    \n",
    "BNSP_svc_mean = np.mean(BNSP_svc)\n",
    "print('BNSP is:')\n",
    "print(BNSP_svc_mean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOG#\n",
    "############Final Metric ####################\n",
    "score_log = 0.25*log_score_noidentity + 0.25*subgroupAUC_log_mean + 0.25*BPSN_log_mean + 0.25*BNSP_log_mean\n",
    "print('Final Metric is:')\n",
    "print(score_log)\n",
    "\n",
    "\n",
    "print('with p = -5')\n",
    "print('overall auc is')\n",
    "subgroupAUC_log_inverse = np.divide(1.0, subgroupAUC_log)\n",
    "part1 = (1/np.mean(np.power(subgroupAUC_log_inverse, 5))) ** 0.2\n",
    "print('subgroup auc is')\n",
    "print(part1)\n",
    "BPSN_log_inverse = np.divide(1.0, BPSN_log)\n",
    "part2 = (1/np.mean(np.power(BPSN_log_inverse, 5))) ** 0.2\n",
    "print('bpsn is')\n",
    "print(part2)\n",
    "BNSP_log_inverse = np.divide(1.0, BNSP_log)\n",
    "part3 = (1/np.mean(np.power(BNSP_log_inverse, 5))) ** 0.2\n",
    "print('bnsp is')\n",
    "print(part3)\n",
    "print('final metric is')\n",
    "print(0.25*log_score_noidentity + 0.25*part1 + 0.25*part2 + 0.25*part3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RFC#\n",
    "############Final Metric ####################\n",
    "score_rfc = 0.25*rfc_score_noidentity + 0.25*subgroupAUC_rfc_mean + 0.25*BPSN_rfc_mean + 0.25*BNSP_rfc_mean\n",
    "print('Final Metric is:')\n",
    "print(score_rfc)\n",
    "\n",
    "\n",
    "print('with p = -5')\n",
    "print('overall auc is')\n",
    "subgroupAUC_rfc_inverse = np.divide(1.0, subgroupAUC_rfc)\n",
    "part1 = (1/np.mean(np.power(subgroupAUC_rfc_inverse, 5))) ** 0.2\n",
    "print('subgroup auc is')\n",
    "print(part1)\n",
    "BPSN_rfc_inverse = np.divide(1.0, BPSN_rfc)\n",
    "part2 = (1/np.mean(np.power(BPSN_rfc_inverse, 5))) ** 0.2\n",
    "print('bpsn is')\n",
    "print(part2)\n",
    "BNSP_rfc_inverse = np.divide(1.0, BNSP_rfc)\n",
    "part3 = (1/np.mean(np.power(BNSP_rfc_inverse, 5))) ** 0.2\n",
    "print('bnsp is')\n",
    "print(part3)\n",
    "print('final metric is')\n",
    "print(0.25*rfc_score_noidentity + 0.25*part1 + 0.25*part2 + 0.25*part3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVC#\n",
    "############Final Metric ####################\n",
    "score_svc = 0.25*svc_score_noidentity + 0.25*subgroupAUC_svc_mean + 0.25*BPSN_svc_mean + 0.25*BNSP_svc_mean\n",
    "print('Final Metric is:')\n",
    "print(score_svc)\n",
    "\n",
    "\n",
    "print('with p = -5')\n",
    "print('overall auc is')\n",
    "subgroupAUC_svc_inverse = np.divide(1.0, subgroupAUC_svc)\n",
    "part1 = (1/np.mean(np.power(subgroupAUC_svc_inverse, 5))) ** 0.2\n",
    "print('subgroup auc is')\n",
    "print(part1)\n",
    "BPSN_svc_inverse = np.divide(1.0, BPSN_svc)\n",
    "part2 = (1/np.mean(np.power(BPSN_svc_inverse, 5))) ** 0.2\n",
    "print('bpsn is')\n",
    "print(part2)\n",
    "BNSP_svc_inverse = np.divide(1.0, BNSP_svc)\n",
    "part3 = (1/np.mean(np.power(BNSP_svc_inverse, 5))) ** 0.2\n",
    "print('bnsp is')\n",
    "print(part3)\n",
    "print('final metric is')\n",
    "print(0.25*svc_score_noidentity + 0.25*part1 + 0.25*part2 + 0.25*part3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_asian</th>\n",
       "      <th>_atheist</th>\n",
       "      <th>_bisexual</th>\n",
       "      <th>_black</th>\n",
       "      <th>_buddhist</th>\n",
       "      <th>_christian</th>\n",
       "      <th>_female</th>\n",
       "      <th>_heterosexual</th>\n",
       "      <th>_hindu</th>\n",
       "      <th>_homosexual_gay_or_lesbian</th>\n",
       "      <th>...</th>\n",
       "      <th>wrote</th>\n",
       "      <th>ye</th>\n",
       "      <th>year</th>\n",
       "      <th>yet</th>\n",
       "      <th>young</th>\n",
       "      <th>your</th>\n",
       "      <th>youv</th>\n",
       "      <th>’</th>\n",
       "      <th>“</th>\n",
       "      <th>”</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1014 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   _asian  _atheist  _bisexual  _black  _buddhist  _christian  _female  \\\n",
       "0   False     False      False   False      False       False    False   \n",
       "1   False     False      False   False      False       False    False   \n",
       "2   False     False      False   False      False       False    False   \n",
       "3   False     False      False   False      False       False    False   \n",
       "4   False     False      False   False      False       False    False   \n",
       "\n",
       "   _heterosexual  _hindu  _homosexual_gay_or_lesbian ...  wrote  ye  year  \\\n",
       "0          False   False                       False ...      0   0     0   \n",
       "1          False   False                       False ...      0   0     0   \n",
       "2          False   False                       False ...      0   0     0   \n",
       "3          False   False                       False ...      0   0     0   \n",
       "4          False   False                       False ...      0   0     0   \n",
       "\n",
       "   yet  young  your  youv  ’  “  ”  \n",
       "0    0      0     0     0  0  0  0  \n",
       "1    0      0     0     0  0  0  0  \n",
       "2    0      0     0     1  0  0  0  \n",
       "3    0      0     0     0  0  0  0  \n",
       "4    0      0     1     0  0  0  0  \n",
       "\n",
       "[5 rows x 1014 columns]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_asian\n",
      "_atheist\n",
      "_bisexual\n",
      "_black\n",
      "_buddhist\n",
      "_christian\n",
      "_female\n",
      "_heterosexual\n",
      "_hindu\n",
      "_homosexual_gay_or_lesbian\n",
      "_intellectual_or_learning_disability\n",
      "_jewish\n",
      "_latino\n",
      "_male\n",
      "_muslim\n",
      "_transgender\n",
      "_white\n"
     ]
    }
   ],
   "source": [
    "# X_train_final is the full dataset with both bag of words and identity representation\n",
    "# identity_columns2 is a vector of column names that represent identities\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "predicted_identities = np.zeros((len(X_test_final),len(identity_columns2)))\n",
    "\n",
    "for i in range(len(identity_columns2)):\n",
    "    c = identity_columns2[i]\n",
    "    print(c)\n",
    "    temp_true = X_train_final[X_train_final[c] == True]\n",
    "    n = len(temp_true)\n",
    "    \n",
    "    temp_false = X_train_final[X_train_final[c] == False]\n",
    "    temp_false_sampled = temp_false.loc[random.sample(list(temp_false.index), n)]\n",
    "    \n",
    "    temp_final = pd.concat([temp_true, temp_false_sampled], axis=0)\n",
    "    \n",
    "    # the true identity in X_train we are trying to predict\n",
    "    temp_y = temp_final[c].astype('bool')\n",
    "    # the bag of words corresponding to the selected balanced dataset\n",
    "    temp_X = temp_final.copy().drop(identity_columns2, axis = 1)\n",
    "    \n",
    "    \n",
    "    ##################### Log Predict ##################################################\n",
    "    # fit logistic regression to predict the identity 'c' from the selected balanced dataset\n",
    "    clf_log = LogisticRegression(random_state=0, solver='lbfgs').fit(temp_X, temp_y)\n",
    "    \n",
    "    # predict identity 'c' of the Test dataset, df2 represents the bag of words representation of X_test\n",
    "    predicted_identity = clf_log.predict(X_test_final.copy().drop(identity_columns2, axis = 1))\n",
    "    predicted_identities[:,i] = predicted_identity.astype('bool')\n",
    "    \n",
    " \n",
    "    \n",
    "\n",
    "predicted_identities= pd.DataFrame(predicted_identities, columns = identity_columns2)\n",
    "s = X_train_notext.columns.to_series()\n",
    "predicted_identities.columns = s\n",
    "predicted_identities.reset_index(drop = True, inplace = True)\n",
    "\n",
    "X_test_estimated = pd.concat([predicted_identities, df2], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_asian</th>\n",
       "      <th>_atheist</th>\n",
       "      <th>_bisexual</th>\n",
       "      <th>_black</th>\n",
       "      <th>_buddhist</th>\n",
       "      <th>_christian</th>\n",
       "      <th>_female</th>\n",
       "      <th>_heterosexual</th>\n",
       "      <th>_hindu</th>\n",
       "      <th>_homosexual_gay_or_lesbian</th>\n",
       "      <th>...</th>\n",
       "      <th>wrote</th>\n",
       "      <th>ye</th>\n",
       "      <th>year</th>\n",
       "      <th>yet</th>\n",
       "      <th>young</th>\n",
       "      <th>your</th>\n",
       "      <th>youv</th>\n",
       "      <th>’</th>\n",
       "      <th>“</th>\n",
       "      <th>”</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1014 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   _asian  _atheist  _bisexual  _black  _buddhist  _christian  _female  \\\n",
       "0     0.0       0.0        0.0     0.0        0.0         0.0      0.0   \n",
       "1     0.0       0.0        0.0     0.0        0.0         0.0      0.0   \n",
       "2     0.0       1.0        0.0     0.0        0.0         0.0      0.0   \n",
       "3     0.0       1.0        1.0     0.0        0.0         0.0      0.0   \n",
       "4     0.0       1.0        0.0     0.0        0.0         1.0      0.0   \n",
       "\n",
       "   _heterosexual  _hindu  _homosexual_gay_or_lesbian ...  wrote  ye  year  \\\n",
       "0            0.0     0.0                         0.0 ...      0   0     0   \n",
       "1            0.0     0.0                         0.0 ...      0   0     0   \n",
       "2            1.0     0.0                         0.0 ...      0   0     0   \n",
       "3            1.0     0.0                         0.0 ...      0   0     0   \n",
       "4            0.0     0.0                         0.0 ...      0   0     0   \n",
       "\n",
       "   yet  young  your  youv  ’  “  ”  \n",
       "0    0      0     0     0  0  0  0  \n",
       "1    0      0     0     0  0  0  0  \n",
       "2    0      0     0     1  0  0  0  \n",
       "3    0      0     0     0  0  0  0  \n",
       "4    0      0     1     0  0  0  0  \n",
       "\n",
       "[5 rows x 1014 columns]"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_estimated.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6934326288926116\n",
      "0.7818088042154742\n",
      "0.6589816665957718\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "clf_log_identity = LogisticRegression(random_state=0, solver='lbfgs', class_weight = 'balanced').fit(X_train_final, y_train)\n",
    "    \n",
    "y_predict_log_identity = clf_log_identity.predict(X_test_estimated)\n",
    "\n",
    "log_score_identity = roc_auc_score(y_test, y_predict_log_identity)\n",
    "\n",
    "print(log_score_identity)\n",
    "print(clf_log_identity.score(X_train_final, y_train))\n",
    "print(clf_log_identity.score(X_test_estimated, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall AUC is:\n",
      "0.6934326288926116\n",
      "Subgroup AUC is:\n",
      "0.6560870717184137\n"
     ]
    }
   ],
   "source": [
    "###########compute overall AUC #######\n",
    "print('Overall AUC is:')\n",
    "print(log_score_identity)\n",
    "\n",
    "############compute Subgroup AUC #####\n",
    "subgroupAUC_log = np.zeros(len(identity_columns))\n",
    "\n",
    "for i in range(len(identity_columns2)):\n",
    "    c = identity_columns2[i]\n",
    "    #print(c)\n",
    "    temp1 = X_test_final[X_test_final[c] == True]\n",
    "    temp = X_test_estimated.loc[list(temp1.index)]\n",
    "    #print(temp.shape)\n",
    "    if len(temp) == 0:\n",
    "        subgroupAUC_log[i] = 1\n",
    "    else:\n",
    "        y_predict_sub1 = clf_log_identity.predict(temp)\n",
    "        y_test_sub1 = y_test[list(temp.index)]\n",
    "        log_score_sub1 = roc_auc_score(y_test_sub1, y_predict_sub1)\n",
    "        subgroupAUC_log[i] = log_score_sub1\n",
    "\n",
    "            \n",
    "    \n",
    "subgroupAUC_log_mean = np.mean(subgroupAUC_log)\n",
    "print('Subgroup AUC is:')\n",
    "print(subgroupAUC_log_mean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BPSN is:\n",
      "0.5904212832834496\n"
     ]
    }
   ],
   "source": [
    "############compute BPSN Background Positive, Subgroup Negative AUC#####\n",
    "BPSN_log = np.zeros(len(identity_columns))\n",
    "y_test1 = pd.DataFrame(y_test)\n",
    "y_test1.columns = ['toxicity']\n",
    "for i in range(len(identity_columns2)):\n",
    "    c = identity_columns2[i]\n",
    "    temp_pt1 = X_test_estimated.loc[(y_test1['toxicity'] == False) & (X_test_final[c] == True)]\n",
    "    temp_pt2 = X_test_estimated.loc[(y_test1['toxicity'] == True) & (X_test_final[c] == False)]\n",
    "    \n",
    "    \n",
    "    X_test_final_sub2 = pd.concat([temp_pt1, temp_pt2], axis = 0)\n",
    "    \n",
    "    y_predict_sub2 = clf_log_identity.predict(X_test_final_sub2)\n",
    "    y_test_sub2 = y_test[list(X_test_final_sub2.index)]\n",
    "    \n",
    "    log_score_sub2 = roc_auc_score(y_test_sub2, y_predict_sub2)\n",
    "    BPSN_log[i] = log_score_sub2\n",
    "    \n",
    "BPSN_log_mean = np.mean(BPSN_log)\n",
    "print('BPSN is:')\n",
    "print(BPSN_log_mean)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BNSP is:\n",
      "0.7579372243836852\n"
     ]
    }
   ],
   "source": [
    "############compute BNSP Background Negative, Subgroup Positive AUC#####\n",
    "BNSP_log = np.zeros(len(identity_columns))\n",
    "for i in range(len(identity_columns2)):\n",
    "    c = identity_columns2[i]\n",
    "    \n",
    "    temp_pt1 = X_test_estimated.loc[(y_test1['toxicity'] == True) & (X_test_final[c] == True)]\n",
    "    temp_pt2 = X_test_estimated.loc[(y_test1['toxicity'] == False) & (X_test_final[c] == False)]\n",
    "\n",
    "    \n",
    "    X_test_final_sub3 = pd.concat([temp_pt1, temp_pt2], axis = 0)\n",
    "    \n",
    "    y_predict_sub3 = clf_log_identity.predict(X_test_final_sub3)\n",
    "    y_test_sub3 = y_test[list(X_test_final_sub3.index)]\n",
    "    \n",
    "    log_score_sub3 = roc_auc_score(y_test_sub3, y_predict_sub3)\n",
    "    BNSP_log[i] = log_score_sub3\n",
    "    \n",
    "BNSP_log_mean = np.mean(BNSP_log)\n",
    "print('BNSP is:')\n",
    "print(BNSP_log_mean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Metric is:\n",
      "0.6801034535445485\n",
      "with p = -5\n",
      "overall auc is\n",
      "subgroup auc is\n",
      "0.6323676755902574\n",
      "bpsn is\n",
      "0.5419928470351245\n",
      "bnsp is\n",
      "0.7486216616619104\n",
      "final metric is\n",
      "0.6541037032949759\n"
     ]
    }
   ],
   "source": [
    "############Final Metric ####################\n",
    "score_log = 0.25*log_score_noidentity + 0.25*subgroupAUC_log_mean + 0.25*BPSN_log_mean + 0.25*BNSP_log_mean\n",
    "print('Final Metric is:')\n",
    "print(score_log)\n",
    "\n",
    "\n",
    "print('with p = -5')\n",
    "print('overall auc is')\n",
    "subgroupAUC_log_inverse = np.divide(1.0, subgroupAUC_log)\n",
    "part1 = (1/np.mean(np.power(subgroupAUC_log_inverse, 5))) ** 0.2\n",
    "print('subgroup auc is')\n",
    "print(part1)\n",
    "BPSN_log_inverse = np.divide(1.0, BPSN_log)\n",
    "part2 = (1/np.mean(np.power(BPSN_log_inverse, 5))) ** 0.2\n",
    "print('bpsn is')\n",
    "print(part2)\n",
    "BNSP_log_inverse = np.divide(1.0, BNSP_log)\n",
    "part3 = (1/np.mean(np.power(BNSP_log_inverse, 5))) ** 0.2\n",
    "print('bnsp is')\n",
    "print(part3)\n",
    "print('final metric is')\n",
    "print(0.25*log_score_identity + 0.25*part1 + 0.25*part2 + 0.25*part3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6934326288926116\n",
      "0.793097214320276\n",
      "0.6645540005955166\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "clf_SVC_identity = LinearSVC(class_weight = 'balanced').fit(X_train_final, y_train)\n",
    "y_predict_SVC_identity = clf_SVC_identity.predict(X_test_estimated)\n",
    "SVC_score_identity = roc_auc_score(y_test, y_predict_log_identity)\n",
    "\n",
    "print(SVC_score_identity)\n",
    "print(clf_SVC_identity.score(X_train_final, y_train))\n",
    "print(clf_SVC_identity.score(X_test_estimated, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall AUC is:\n",
      "0.6934326288926116\n",
      "Subgroup AUC is:\n",
      "0.6527879103240851\n",
      "BPSN is:\n",
      "0.5851957837893274\n",
      "BNSP is:\n",
      "0.7592490261010133\n",
      "Final Metric is:\n",
      "0.6726663372767594\n",
      "with p = -5\n",
      "overall auc is\n",
      "subgroup auc is\n",
      "0.6302810484996727\n",
      "bpsn is\n",
      "0.5367802597077723\n",
      "bnsp is\n",
      "0.7498143008937661\n",
      "final metric is\n",
      "0.6525770594984557\n"
     ]
    }
   ],
   "source": [
    "###########compute overall AUC #######\n",
    "print('Overall AUC is:')\n",
    "print(SVC_score_identity)\n",
    "\n",
    "############compute Subgroup AUC #####\n",
    "subgroupAUC_SVC = np.zeros(len(identity_columns))\n",
    "\n",
    "for i in range(len(identity_columns2)):\n",
    "    c = identity_columns2[i]\n",
    "    #print(c)\n",
    "    temp1 = X_test_final[X_test_final[c] == True]\n",
    "    temp = X_test_estimated.loc[list(temp1.index)]\n",
    "    #print(temp.shape)\n",
    "    if len(temp) == 0:\n",
    "        subgroupAUC_SVC[i] = 1\n",
    "    else:\n",
    "        y_predict_sub1 = clf_SVC_identity.predict(temp)\n",
    "        y_test_sub1 = y_test[list(temp.index)]\n",
    "        SVC_score_sub1 = roc_auc_score(y_test_sub1, y_predict_sub1)\n",
    "        subgroupAUC_SVC[i] = SVC_score_sub1\n",
    "\n",
    "            \n",
    "    \n",
    "subgroupAUC_SVC_mean = np.mean(subgroupAUC_SVC)\n",
    "print('Subgroup AUC is:')\n",
    "print(subgroupAUC_SVC_mean)\n",
    "\n",
    "############compute BPSN Background Positive, Subgroup Negative AUC#####\n",
    "BPSN_SVC = np.zeros(len(identity_columns))\n",
    "y_test1 = pd.DataFrame(y_test)\n",
    "y_test1.columns = ['toxicity']\n",
    "for i in range(len(identity_columns2)):\n",
    "    c = identity_columns2[i]\n",
    "    temp_pt1 = X_test_estimated.loc[(y_test1['toxicity'] == False) & (X_test_final[c] == True)]\n",
    "    temp_pt2 = X_test_estimated.loc[(y_test1['toxicity'] == True) & (X_test_final[c] == False)]\n",
    "    \n",
    "    \n",
    "    X_test_final_sub2 = pd.concat([temp_pt1, temp_pt2], axis = 0)\n",
    "    \n",
    "    y_predict_sub2 = clf_SVC_identity.predict(X_test_final_sub2)\n",
    "    y_test_sub2 = y_test[list(X_test_final_sub2.index)]\n",
    "    \n",
    "    SVC_score_sub2 = roc_auc_score(y_test_sub2, y_predict_sub2)\n",
    "    BPSN_SVC[i] = SVC_score_sub2\n",
    "    \n",
    "BPSN_SVC_mean = np.mean(BPSN_SVC)\n",
    "print('BPSN is:')\n",
    "print(BPSN_SVC_mean)\n",
    "    \n",
    "############compute BNSP Background Negative, Subgroup Positive AUC#####\n",
    "BNSP_SVC = np.zeros(len(identity_columns))\n",
    "for i in range(len(identity_columns2)):\n",
    "    c = identity_columns2[i]\n",
    "    \n",
    "    temp_pt1 = X_test_estimated.loc[(y_test1['toxicity'] == True) & (X_test_final[c] == True)]\n",
    "    temp_pt2 = X_test_estimated.loc[(y_test1['toxicity'] == False) & (X_test_final[c] == False)]\n",
    "\n",
    "    \n",
    "    X_test_final_sub3 = pd.concat([temp_pt1, temp_pt2], axis = 0)\n",
    "    \n",
    "    y_predict_sub3 = clf_SVC_identity.predict(X_test_final_sub3)\n",
    "    y_test_sub3 = y_test[list(X_test_final_sub3.index)]\n",
    "    \n",
    "    SVC_score_sub3 = roc_auc_score(y_test_sub3, y_predict_sub3)\n",
    "    BNSP_SVC[i] = SVC_score_sub3\n",
    "    \n",
    "BNSP_SVC_mean = np.mean(BNSP_SVC)\n",
    "print('BNSP is:')\n",
    "print(BNSP_SVC_mean)\n",
    "\n",
    "############Final Metric ####################\n",
    "score_SVC = 0.25*SVC_score_identity + 0.25*subgroupAUC_SVC_mean + 0.25*BPSN_SVC_mean + 0.25*BNSP_SVC_mean\n",
    "print('Final Metric is:')\n",
    "print(score_SVC)\n",
    "\n",
    "\n",
    "print('with p = -5')\n",
    "print('overall auc is')\n",
    "subgroupAUC_SVC_inverse = np.divide(1.0, subgroupAUC_SVC)\n",
    "part1 = (1/np.mean(np.power(subgroupAUC_SVC_inverse, 5))) ** 0.2\n",
    "print('subgroup auc is')\n",
    "print(part1)\n",
    "BPSN_SVC_inverse = np.divide(1.0, BPSN_SVC)\n",
    "part2 = (1/np.mean(np.power(BPSN_SVC_inverse, 5))) ** 0.2\n",
    "print('bpsn is')\n",
    "print(part2)\n",
    "BNSP_SVC_inverse = np.divide(1.0, BNSP_SVC)\n",
    "part3 = (1/np.mean(np.power(BNSP_SVC_inverse, 5))) ** 0.2\n",
    "print('bnsp is')\n",
    "print(part3)\n",
    "print('final metric is')\n",
    "print(0.25*SVC_score_identity + 0.25*part1 + 0.25*part2 + 0.25*part3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5554915014766049\n",
      "0.9908703720443028\n",
      "0.8849589518907652\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "clf_RF_identity = RandomForestClassifier(n_estimators=50, random_state=0, class_weight = 'balanced')\n",
    "clf_RF_identity.fit(X_train_final, y_train) \n",
    "\n",
    "y_predict_RF_identity = clf_RF_identity.predict(X_test_estimated)\n",
    "RF_score_identity = roc_auc_score(y_test, y_predict_RF_identity)\n",
    "\n",
    "print(RF_score_identity)\n",
    "print(clf_RF_identity.score(X_train_final, y_train))\n",
    "print(clf_RF_identity.score(X_test_estimated, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall AUC is:\n",
      "0.5554915014766049\n",
      "Subgroup AUC is:\n",
      "0.5310262855649387\n",
      "BPSN is:\n",
      "0.5571816150595875\n",
      "BNSP is:\n",
      "0.5299833447118536\n",
      "Final Metric is:\n",
      "0.5434206867032463\n",
      "with p = -5\n",
      "overall auc is\n",
      "subgroup auc is\n",
      "0.5274653726319837\n",
      "bpsn is\n",
      "0.556334094987835\n",
      "bnsp is\n",
      "0.5270890078649004\n",
      "final metric is\n",
      "0.541594994240331\n"
     ]
    }
   ],
   "source": [
    "###########compute overall AUC #######\n",
    "print('Overall AUC is:')\n",
    "print(RF_score_identity)\n",
    "\n",
    "############compute Subgroup AUC #####\n",
    "subgroupAUC_RF = np.zeros(len(identity_columns))\n",
    "\n",
    "for i in range(len(identity_columns2)):\n",
    "    c = identity_columns2[i]\n",
    "    #print(c)\n",
    "    temp1 = X_test_final[X_test_final[c] == True]\n",
    "    temp = X_test_estimated.loc[list(temp1.index)]\n",
    "    #print(temp.shape)\n",
    "    if len(temp) == 0:\n",
    "        subgroupAUC_RF[i] = 1\n",
    "    else:\n",
    "        y_predict_sub1 = clf_RF_identity.predict(temp)\n",
    "        y_test_sub1 = y_test[list(temp.index)]\n",
    "        RF_score_sub1 = roc_auc_score(y_test_sub1, y_predict_sub1)\n",
    "        subgroupAUC_RF[i] = RF_score_sub1\n",
    "\n",
    "            \n",
    "    \n",
    "subgroupAUC_RF_mean = np.mean(subgroupAUC_RF)\n",
    "print('Subgroup AUC is:')\n",
    "print(subgroupAUC_RF_mean)\n",
    "\n",
    "############compute BPSN Background Positive, Subgroup Negative AUC#####\n",
    "BPSN_RF = np.zeros(len(identity_columns))\n",
    "y_test1 = pd.DataFrame(y_test)\n",
    "y_test1.columns = ['toxicity']\n",
    "for i in range(len(identity_columns2)):\n",
    "    c = identity_columns2[i]\n",
    "    temp_pt1 = X_test_estimated.loc[(y_test1['toxicity'] == False) & (X_test_final[c] == True)]\n",
    "    temp_pt2 = X_test_estimated.loc[(y_test1['toxicity'] == True) & (X_test_final[c] == False)]\n",
    "    \n",
    "    \n",
    "    X_test_final_sub2 = pd.concat([temp_pt1, temp_pt2], axis = 0)\n",
    "    \n",
    "    y_predict_sub2 = clf_RF_identity.predict(X_test_final_sub2)\n",
    "    y_test_sub2 = y_test[list(X_test_final_sub2.index)]\n",
    "    \n",
    "    RF_score_sub2 = roc_auc_score(y_test_sub2, y_predict_sub2)\n",
    "    BPSN_RF[i] = RF_score_sub2\n",
    "    \n",
    "BPSN_RF_mean = np.mean(BPSN_RF)\n",
    "print('BPSN is:')\n",
    "print(BPSN_RF_mean)\n",
    "    \n",
    "############compute BNSP Background Negative, Subgroup Positive AUC#####\n",
    "BNSP_RF = np.zeros(len(identity_columns))\n",
    "for i in range(len(identity_columns2)):\n",
    "    c = identity_columns2[i]\n",
    "    \n",
    "    temp_pt1 = X_test_estimated.loc[(y_test1['toxicity'] == True) & (X_test_final[c] == True)]\n",
    "    temp_pt2 = X_test_estimated.loc[(y_test1['toxicity'] == False) & (X_test_final[c] == False)]\n",
    "\n",
    "    \n",
    "    X_test_final_sub3 = pd.concat([temp_pt1, temp_pt2], axis = 0)\n",
    "    \n",
    "    y_predict_sub3 = clf_RF_identity.predict(X_test_final_sub3)\n",
    "    y_test_sub3 = y_test[list(X_test_final_sub3.index)]\n",
    "    \n",
    "    RF_score_sub3 = roc_auc_score(y_test_sub3, y_predict_sub3)\n",
    "    BNSP_RF[i] = RF_score_sub3\n",
    "    \n",
    "BNSP_RF_mean = np.mean(BNSP_RF)\n",
    "print('BNSP is:')\n",
    "print(BNSP_RF_mean)\n",
    "\n",
    "############Final Metric ####################\n",
    "score_RF = 0.25*RF_score_identity + 0.25*subgroupAUC_RF_mean + 0.25*BPSN_RF_mean + 0.25*BNSP_RF_mean\n",
    "print('Final Metric is:')\n",
    "print(score_RF)\n",
    "\n",
    "\n",
    "print('with p = -5')\n",
    "print('overall auc is')\n",
    "subgroupAUC_RF_inverse = np.divide(1.0, subgroupAUC_RF)\n",
    "part1 = (1/np.mean(np.power(subgroupAUC_RF_inverse, 5))) ** 0.2\n",
    "print('subgroup auc is')\n",
    "print(part1)\n",
    "BPSN_RF_inverse = np.divide(1.0, BPSN_RF)\n",
    "part2 = (1/np.mean(np.power(BPSN_RF_inverse, 5))) ** 0.2\n",
    "print('bpsn is')\n",
    "print(part2)\n",
    "BNSP_RF_inverse = np.divide(1.0, BNSP_RF)\n",
    "part3 = (1/np.mean(np.power(BNSP_RF_inverse, 5))) ** 0.2\n",
    "print('bnsp is')\n",
    "print(part3)\n",
    "print('final metric is')\n",
    "print(0.25*RF_score_identity + 0.25*part1 + 0.25*part2 + 0.25*part3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
